import pandas as pd
import numpy as np
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    silhouette_score,
    calinski_harabasz_score,
    davies_bouldin_score,
)
from sklearn.neighbors import NearestNeighbors
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import warnings
import sys

# Configurar encoding para UTF-8
if sys.platform == "win32":
    sys.stdout.reconfigure(encoding='utf-8')

warnings.filterwarnings("ignore")


class DBSCANEpithelialClusterer:
    def __init__(self, eps=0.5, min_samples=5, random_state=42):
        """
        Inicializa o algoritmo DBSCAN para an√°lise de espessura epitelial

        Args:
            eps (float): Dist√¢ncia m√°xima entre dois pontos para serem considerados vizinhos
            min_samples (int): N√∫mero m√≠nimo de amostras em uma vizinhan√ßa para um ponto ser considerado core
            random_state (int): Semente para reprodutibilidade
        """
        self.eps = eps
        self.min_samples = min_samples
        self.random_state = random_state
        self.dbscan = None
        self.scaler = StandardScaler()
        self.data = None
        self.features_data = None
        self.scaled_data = None
        self.labels = None

    def load_data(self, file_path):
        """
        Carrega os dados do arquivo CSV

        Args:
            file_path (str): Caminho para o arquivo CSV
        """
        self.data = pd.read_csv(file_path)
        print(
            f"Dados carregados: {self.data.shape[0]} amostras, {self.data.shape[1]} caracter√≠sticas"
        )
        print(f"Valores ausentes por coluna:\n{self.data.isnull().sum()}")

    def preprocess_data(self):
        """
        Preprocessa os dados: seleciona apenas as vari√°veis de espessura epitelial e normaliza
        Remove linhas com valores ausentes nas features de interesse
        """
        if self.data is None:
            raise ValueError(
                "Os dados ainda n√£o foram carregados. Use load_data primeiro."
            )

        # Seleciona apenas as vari√°veis de espessura epitelial
        features = ["C", "S", "ST", "T", "IT", "I", "IN", "N", "SN"]
        self.features_data = self.data[features].copy()

        # Remove linhas com valores ausentes
        print(f"\nLinhas antes de remover NaN: {len(self.features_data)}")
        self.features_data = self.features_data.dropna()
        print(f"Linhas ap√≥s remover NaN: {len(self.features_data)}")

        # Normaliza os dados
        self.scaled_data = self.scaler.fit_transform(self.features_data)

        print("\nDados preprocessados e normalizados")
        print(f"Caracter√≠sticas utilizadas: {features}")
        print(f"Estat√≠sticas descritivas:")
        print(self.features_data.describe())

    def find_optimal_eps(self, k=5):
        """
        Encontra o valor √≥timo de eps usando o m√©todo do cotovelo com k-distance

        Args:
            k (int): N√∫mero de vizinhos para calcular a dist√¢ncia

        Returns:
            float: Valor sugerido de eps
        """
        if self.scaled_data is None:
            raise ValueError(
                "Os dados ainda n√£o foram preprocessados. Use preprocess_data primeiro."
            )

        print(f"\nüîç Calculando k-distance para k={k}...")

        # Calcula as k dist√¢ncias mais pr√≥ximas
        neighbors = NearestNeighbors(n_neighbors=k)
        neighbors_fit = neighbors.fit(self.scaled_data)
        distances, indices = neighbors_fit.kneighbors(self.scaled_data)

        # Ordena as dist√¢ncias
        distances = np.sort(distances[:, k - 1], axis=0)

        # Visualiza o gr√°fico de k-distance
        plt.figure(figsize=(10, 6))
        plt.plot(distances)
        plt.xlabel("Pontos ordenados")
        plt.ylabel(f"{k}-distance")
        plt.title(f"K-distance Graph (k={k})\nProcure pelo 'cotovelo' para determinar eps")
        plt.grid(True, alpha=0.3)
        plt.tight_layout()

        # Salva a figura
        results_dir = Path("results")
        results_dir.mkdir(exist_ok=True)
        plt.savefig(results_dir / "dbscan_eps_optimization.png", dpi=300, bbox_inches="tight")
        plt.close()

        # Sugest√£o de eps (90¬∫ percentil como heur√≠stica)
        suggested_eps = np.percentile(distances, 90)
        print(f"üìä Valor sugerido de eps (90¬∫ percentil): {suggested_eps:.4f}")

        return suggested_eps

    def optimize_parameters(self, eps_range=None, min_samples_range=None):
        """
        Testa diferentes combina√ß√µes de eps e min_samples

        Args:
            eps_range (list): Lista de valores de eps para testar
            min_samples_range (list): Lista de valores de min_samples para testar

        Returns:
            DataFrame: Resultados da otimiza√ß√£o
        """
        if self.scaled_data is None:
            raise ValueError(
                "Os dados ainda n√£o foram preprocessados. Use preprocess_data primeiro."
            )

        if eps_range is None:
            eps_range = [0.3, 0.5, 0.7, 1.0, 1.5, 2.0]
        if min_samples_range is None:
            min_samples_range = [3, 5, 10, 15, 20]

        results = []

        print(f"\nüîç Testando combina√ß√µes de par√¢metros...")

        for eps in eps_range:
            for min_samples in min_samples_range:
                dbscan = DBSCAN(eps=eps, min_samples=min_samples)
                labels = dbscan.fit_predict(self.scaled_data)

                n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
                n_noise = list(labels).count(-1)
                noise_pct = (n_noise / len(labels)) * 100

                result = {
                    "eps": eps,
                    "min_samples": min_samples,
                    "n_clusters": n_clusters,
                    "n_noise": n_noise,
                    "noise_pct": noise_pct,
                }

                # Calcula m√©tricas apenas se houver pelo menos 2 clusters
                if n_clusters >= 2 and n_noise < len(labels):
                    # Remove ru√≠do para c√°lculo de m√©tricas
                    mask = labels != -1
                    if sum(mask) > 0:
                        try:
                            result["silhouette"] = silhouette_score(
                                self.scaled_data[mask], labels[mask]
                            )
                            result["calinski_harabasz"] = calinski_harabasz_score(
                                self.scaled_data[mask], labels[mask]
                            )
                            result["davies_bouldin"] = davies_bouldin_score(
                                self.scaled_data[mask], labels[mask]
                            )
                        except:
                            result["silhouette"] = np.nan
                            result["calinski_harabasz"] = np.nan
                            result["davies_bouldin"] = np.nan
                else:
                    result["silhouette"] = np.nan
                    result["calinski_harabasz"] = np.nan
                    result["davies_bouldin"] = np.nan

                results.append(result)

                silhouette_str = f"{result['silhouette']:.3f}" if not np.isnan(result['silhouette']) else 'N/A'
                print(
                    f"  eps={eps:.2f}, min_samples={min_samples}: "
                    f"{n_clusters} clusters, {n_noise} ru√≠dos ({noise_pct:.1f}%), "
                    f"Silhouette={silhouette_str}"
                )

        return pd.DataFrame(results)

    def fit(self):
        """
        Treina o modelo DBSCAN com os par√¢metros especificados
        """
        if self.scaled_data is None:
            raise ValueError(
                "Os dados ainda n√£o foram preprocessados. Use preprocess_data primeiro."
            )

        print(f"\nüéØ Treinando DBSCAN com eps={self.eps}, min_samples={self.min_samples}...")
        self.dbscan = DBSCAN(eps=self.eps, min_samples=self.min_samples)
        self.labels = self.dbscan.fit_predict(self.scaled_data)

        # Calcula estat√≠sticas
        n_clusters = len(set(self.labels)) - (1 if -1 in self.labels else 0)
        n_noise = list(self.labels).count(-1)
        noise_pct = (n_noise / len(self.labels)) * 100

        print(f"\nüìä Resultados do DBSCAN:")
        print(f"  N√∫mero de clusters: {n_clusters}")
        print(f"  N√∫mero de ru√≠dos: {n_noise} ({noise_pct:.2f}%)")

        # Calcula m√©tricas apenas se houver pelo menos 2 clusters e pontos n√£o-ru√≠do
        if n_clusters >= 2 and n_noise < len(self.labels):
            mask = self.labels != -1
            if sum(mask) > 0:
                try:
                    silhouette = silhouette_score(self.scaled_data[mask], self.labels[mask])
                    calinski = calinski_harabasz_score(self.scaled_data[mask], self.labels[mask])
                    davies = davies_bouldin_score(self.scaled_data[mask], self.labels[mask])

                    print(f"\nüìä M√©tricas do modelo (sem ru√≠do):")
                    print(f"  Silhouette Score: {silhouette:.4f}")
                    print(f"  Calinski-Harabasz Score: {calinski:.2f}")
                    print(f"  Davies-Bouldin Score: {davies:.4f}")
                except Exception as e:
                    print(f"‚ö†Ô∏è Erro ao calcular m√©tricas: {e}")

        return self

    def get_cluster_profiles(self):
        """
        Retorna o perfil de cada cluster

        Returns:
            tuple: (DataFrame com estat√≠sticas dos clusters, Series com contagens)
        """
        if self.labels is None:
            raise ValueError("O modelo ainda n√£o foi treinado. Use fit primeiro.")

        # Adiciona labels aos dados
        data_with_clusters = self.features_data.copy()
        data_with_clusters["Cluster"] = self.labels

        # Separa ru√≠do dos clusters
        cluster_counts = data_with_clusters["Cluster"].value_counts().sort_index()

        print(f"\nüìà Distribui√ß√£o de amostras por cluster:")
        for cluster, count in cluster_counts.items():
            percentage = (count / len(data_with_clusters)) * 100
            label = "Ru√≠do" if cluster == -1 else f"Cluster {cluster}"
            print(f"  {label}: {count} amostras ({percentage:.1f}%)")

        # Calcula estat√≠sticas por cluster (excluindo ru√≠do)
        data_no_noise = data_with_clusters[data_with_clusters["Cluster"] != -1]
        if len(data_no_noise) > 0:
            cluster_stats = data_no_noise.groupby("Cluster").agg(["mean", "std", "min", "max"])
        else:
            cluster_stats = None

        return cluster_stats, cluster_counts

    def save_results(self, output_path="results/dbscan_results.csv"):
        """
        Salva os resultados em um arquivo CSV

        Args:
            output_path (str): Caminho para salvar os resultados
        """
        if self.labels is None:
            raise ValueError("O modelo ainda n√£o foi treinado. Use fit primeiro.")

        # Combina features com labels
        results_df = self.features_data.copy()
        results_df["Cluster"] = self.labels

        # Adiciona as colunas originais de volta
        original_cols = ["Index", "pID", "Age", "Gender", "Eye"]
        for col in original_cols:
            if col in self.data.columns:
                # Filtra apenas os √≠ndices que n√£o foram removidos
                results_df[col] = self.data.loc[results_df.index, col].values

        # Salva
        results_dir = Path(output_path).parent
        results_dir.mkdir(exist_ok=True)
        results_df.to_csv(output_path, index=False)

        print(f"\nüíæ Resultados salvos em: {output_path}")

        return results_df


if __name__ == "__main__":
    # Exemplo de uso
    clusterer = DBSCANEpithelialClusterer()
    clusterer.load_data("data/RTVue_20221110_MLClass.csv")
    clusterer.preprocess_data()

    # Encontra eps √≥timo
    suggested_eps = clusterer.find_optimal_eps(k=5)

    # Otimiza par√¢metros
    optimization_results = clusterer.optimize_parameters()
    print("\nüìä Top 10 melhores configura√ß√µes:")
    print(
        optimization_results.dropna(subset=["silhouette"])
        .nlargest(10, "silhouette")[
            ["eps", "min_samples", "n_clusters", "noise_pct", "silhouette"]
        ]
    )

    # Treina com os melhores par√¢metros
    best_config = optimization_results.dropna(subset=["silhouette"]).nlargest(1, "silhouette").iloc[0]
    clusterer.eps = best_config["eps"]
    clusterer.min_samples = int(best_config["min_samples"])

    clusterer.fit()
    cluster_stats, cluster_counts = clusterer.get_cluster_profiles()
    clusterer.save_results()
