"""
Script de An√°lise Diagn√≥stica dos Dados de Espessura Epitelial
Objetivo: Entender por que os algoritmos de clustering n√£o est√£o encontrando perfis distintos
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from pathlib import Path
import sys

# Configurar encoding para UTF-8
if sys.platform == "win32":
    sys.stdout.reconfigure(encoding='utf-8')

# Configura√ß√µes
sns.set_style("whitegrid")
plt.rcParams["figure.figsize"] = (14, 10)

# Carrega os dados
print("=" * 80)
print("AN√ÅLISE DIAGN√ìSTICA - DADOS DE ESPESSURA EPITELIAL")
print("=" * 80)

data = pd.read_csv("data/RTVue_20221110_MLClass.csv")
features = ["C", "S", "ST", "T", "IT", "I", "IN", "N", "SN"]
df_features = data[features].dropna()

print(f"\nüìä Informa√ß√µes Gerais:")
print(f"  - Total de amostras: {len(df_features)}")
print(f"  - N√∫mero de features: {len(features)}")

# 1. ESTAT√çSTICAS DESCRITIVAS
print("\n" + "=" * 80)
print("1. ESTAT√çSTICAS DESCRITIVAS DAS FEATURES")
print("=" * 80)
print(df_features.describe())

# 2. AN√ÅLISE DE VARIABILIDADE
print("\n" + "=" * 80)
print("2. AN√ÅLISE DE VARIABILIDADE")
print("=" * 80)

cv = df_features.std() / df_features.mean() * 100  # Coeficiente de varia√ß√£o
print(f"\nCoeficiente de Varia√ß√£o (%):")
for feat in features:
    print(f"  {feat:>3}: {cv[feat]:>6.2f}%")
print(f"\n  M√©dia do CV: {cv.mean():.2f}%")

# 3. AN√ÅLISE DE OUTLIERS
print("\n" + "=" * 80)
print("3. AN√ÅLISE DE OUTLIERS (usando IQR)")
print("=" * 80)

outliers_count = {}
for col in features:
    Q1 = df_features[col].quantile(0.25)
    Q3 = df_features[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = df_features[(df_features[col] < lower_bound) | (df_features[col] > upper_bound)][col]
    outliers_count[col] = len(outliers)
    if len(outliers) > 0:
        print(f"\n{col}: {len(outliers)} outliers ({len(outliers)/len(df_features)*100:.2f}%)")
        print(f"  Valores: min={outliers.min()}, max={outliers.max()}")
        print(f"  Range normal: [{lower_bound:.2f}, {upper_bound:.2f}]")

total_outliers = sum(outliers_count.values())
print(f"\nTotal de outliers detectados: {total_outliers}")

# 4. AN√ÅLISE DE CORRELA√á√ÉO
print("\n" + "=" * 80)
print("4. AN√ÅLISE DE CORRELA√á√ÉO ENTRE FEATURES")
print("=" * 80)

corr_matrix = df_features.corr()
print("\nM√©dia das correla√ß√µes (excluindo diagonal):")
mean_corr = (corr_matrix.sum().sum() - len(features)) / (len(features) * (len(features) - 1))
print(f"  {mean_corr:.4f}")

print("\nMaiores correla√ß√µes:")
corr_pairs = []
for i in range(len(features)):
    for j in range(i + 1, len(features)):
        corr_pairs.append((features[i], features[j], corr_matrix.iloc[i, j]))
corr_pairs.sort(key=lambda x: abs(x[2]), reverse=True)
for feat1, feat2, corr in corr_pairs[:5]:
    print(f"  {feat1} <-> {feat2}: {corr:.4f}")

# 5. AN√ÅLISE DE DISTRIBUI√á√ÉO
print("\n" + "=" * 80)
print("5. AN√ÅLISE DE DISTRIBUI√á√ÉO (Teste de Normalidade)")
print("=" * 80)

for col in features:
    stat, p_value = stats.shapiro(df_features[col])
    is_normal = "‚úì Normal" if p_value > 0.05 else "‚úó N√£o-normal"
    print(f"  {col:>3}: p-value = {p_value:.6f} {is_normal}")

# 6. AN√ÅLISE DE HOMOGENEIDADE
print("\n" + "=" * 80)
print("6. AN√ÅLISE DE HOMOGENEIDADE DOS DADOS")
print("=" * 80)

# Normaliza os dados
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df_features)

# Calcula a dist√¢ncia de cada ponto ao centroide
centroid = scaled_data.mean(axis=0)
distances = np.sqrt(((scaled_data - centroid) ** 2).sum(axis=1))

print(f"\nDist√¢ncias ao centroide:")
print(f"  M√©dia: {distances.mean():.4f}")
print(f"  Desvio padr√£o: {distances.std():.4f}")
print(f"  M√≠nima: {distances.min():.4f}")
print(f"  M√°xima: {distances.max():.4f}")

# Percentis de dist√¢ncia
percentiles = [25, 50, 75, 90, 95, 99]
print(f"\nPercentis de dist√¢ncia:")
for p in percentiles:
    print(f"  {p}¬∫: {np.percentile(distances, p):.4f}")

# 7. AN√ÅLISE PCA
print("\n" + "=" * 80)
print("7. AN√ÅLISE DE COMPONENTES PRINCIPAIS (PCA)")
print("=" * 80)

pca = PCA()
pca.fit(scaled_data)

print(f"\nVari√¢ncia explicada por componente:")
cumulative_var = 0
for i, var in enumerate(pca.explained_variance_ratio_):
    cumulative_var += var
    print(f"  PC{i+1}: {var*100:.2f}% (acumulado: {cumulative_var*100:.2f}%)")

# 8. VISUALIZA√á√ïES
print("\n" + "=" * 80)
print("8. GERANDO VISUALIZA√á√ïES DIAGN√ìSTICAS")
print("=" * 80)

results_dir = Path("results")
results_dir.mkdir(exist_ok=True)

# 8.1. Distribui√ß√µes
fig, axes = plt.subplots(3, 3, figsize=(18, 15))
fig.suptitle("Distribui√ß√µes das Features de Espessura Epitelial", fontsize=16, fontweight="bold")

for idx, col in enumerate(features):
    ax = axes[idx // 3, idx % 3]
    ax.hist(df_features[col], bins=50, edgecolor="black", alpha=0.7)
    ax.set_title(f"{col} (Œº={df_features[col].mean():.1f}, œÉ={df_features[col].std():.1f})")
    ax.set_xlabel("Espessura (Œºm)")
    ax.set_ylabel("Frequ√™ncia")
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(results_dir / "diagnostic_distributions.png", dpi=300, bbox_inches="tight")
plt.close()
print("  ‚úì Distribui√ß√µes salvas")

# 8.2. Boxplots
fig, ax = plt.subplots(figsize=(14, 6))
df_features.boxplot(ax=ax)
ax.set_title("Boxplots das Features de Espessura Epitelial", fontsize=14, fontweight="bold")
ax.set_ylabel("Espessura (Œºm)")
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig(results_dir / "diagnostic_boxplots.png", dpi=300, bbox_inches="tight")
plt.close()
print("  ‚úì Boxplots salvos")

# 8.3. Mapa de correla√ß√£o
fig, ax = plt.subplots(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap="coolwarm", center=0, ax=ax)
ax.set_title("Mapa de Correla√ß√£o entre Features", fontsize=14, fontweight="bold")
plt.tight_layout()
plt.savefig(results_dir / "diagnostic_correlation.png", dpi=300, bbox_inches="tight")
plt.close()
print("  ‚úì Mapa de correla√ß√£o salvo")

# 8.4. PCA
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

# Scree plot
ax1.plot(range(1, len(pca.explained_variance_ratio_) + 1), 
         pca.explained_variance_ratio_ * 100, 'bo-')
ax1.set_xlabel("Componente Principal")
ax1.set_ylabel("Vari√¢ncia Explicada (%)")
ax1.set_title("Scree Plot - Vari√¢ncia Explicada por Componente")
ax1.grid(True, alpha=0.3)

# Scatter plot dos primeiros 2 componentes
pca_2d = PCA(n_components=2).fit_transform(scaled_data)
ax2.scatter(pca_2d[:, 0], pca_2d[:, 1], alpha=0.5, s=10)
ax2.set_xlabel(f"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)")
ax2.set_ylabel(f"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)")
ax2.set_title("Proje√ß√£o nos 2 Primeiros Componentes Principais")
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(results_dir / "diagnostic_pca.png", dpi=300, bbox_inches="tight")
plt.close()
print("  ‚úì An√°lise PCA salva")

# 8.5. Dist√¢ncias ao centroide
fig, ax = plt.subplots(figsize=(12, 6))
ax.hist(distances, bins=50, edgecolor="black", alpha=0.7)
ax.axvline(distances.mean(), color="red", linestyle="--", linewidth=2, label=f"M√©dia: {distances.mean():.2f}")
ax.axvline(np.median(distances), color="green", linestyle="--", linewidth=2, label=f"Mediana: {np.median(distances):.2f}")
ax.set_xlabel("Dist√¢ncia ao Centroide")
ax.set_ylabel("Frequ√™ncia")
ax.set_title("Distribui√ß√£o das Dist√¢ncias ao Centroide (dados normalizados)")
ax.legend()
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig(results_dir / "diagnostic_distances.png", dpi=300, bbox_inches="tight")
plt.close()
print("  ‚úì An√°lise de dist√¢ncias salva")

# 9. CONCLUS√ïES
print("\n" + "=" * 80)
print("9. DIAGN√ìSTICO PRELIMINAR")
print("=" * 80)

print("\nüîç Poss√≠veis raz√µes para clusters desequilibrados:\n")

# Baixa variabilidade
if cv.mean() < 15:
    print("  ‚ö†Ô∏è BAIXA VARIABILIDADE:")
    print(f"     - Coeficiente de varia√ß√£o m√©dio de apenas {cv.mean():.1f}%")
    print("     - Os dados s√£o muito homog√™neos, dificultando a separa√ß√£o em clusters distintos")

# Alta correla√ß√£o
if mean_corr > 0.7:
    print("\n  ‚ö†Ô∏è ALTA CORRELA√á√ÉO ENTRE FEATURES:")
    print(f"     - Correla√ß√£o m√©dia de {mean_corr:.3f}")
    print("     - As features s√£o muito redundantes, n√£o trazendo informa√ß√£o nova")

# Poucos componentes principais necess√°rios
var_90 = np.where(np.cumsum(pca.explained_variance_ratio_) >= 0.90)[0][0] + 1
if var_90 <= 2:
    print(f"\n  ‚ö†Ô∏è BAIXA DIMENSIONALIDADE INTR√çNSECA:")
    print(f"     - Apenas {var_90} componente(s) explicam 90% da vari√¢ncia")
    print("     - Os dados vivem em um espa√ßo de baixa dimensionalidade")

# Distribui√ß√£o das dist√¢ncias
if distances.std() / distances.mean() < 0.3:
    print(f"\n  ‚ö†Ô∏è DADOS MUITO CONCENTRADOS:")
    print(f"     - CV das dist√¢ncias ao centroide: {(distances.std()/distances.mean())*100:.1f}%")
    print("     - Os pontos est√£o muito pr√≥ximos uns dos outros")

# Muitos outliers
if total_outliers > len(df_features) * 0.1:
    print(f"\n  ‚ö†Ô∏è PRESEN√áA DE OUTLIERS:")
    print(f"     - {total_outliers} outliers detectados")
    print("     - Outliers podem estar for√ßando todos os dados normais para um √∫nico cluster")

print("\n" + "=" * 80)
print("An√°lise completa! Visualiza√ß√µes salvas em 'results/'")
print("=" * 80)
