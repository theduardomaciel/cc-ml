# ğŸ“š Metodologia KDD Aplicada

## O que Ã© KDD?

**KDD (Knowledge Discovery in Databases)** Ã© um processo iterativo e interativo de descoberta de conhecimento em bases de dados. Ã‰ a metodologia completa que envolve vÃ¡rias etapas para extrair padrÃµes Ãºteis e compreensÃ­veis dos dados.

## DiferenÃ§a entre KDD e Data Mining

- **KDD**: Processo completo (da seleÃ§Ã£o dos dados Ã  interpretaÃ§Ã£o)
- **Data Mining**: Uma das etapas do KDD (aplicaÃ§Ã£o de algoritmos)

## ğŸ”„ Etapas do KDD Aplicadas Neste Projeto

### 1ï¸âƒ£ SeleÃ§Ã£o de Dados (Data Selection)

**Objetivo**: Identificar e selecionar os dados relevantes para a anÃ¡lise.

**Neste projeto**:
- Dataset: `RTVue_20221110_MLClass.csv`
- VariÃ¡veis selecionadas: 9 medidas de espessura epitelial
  - `C` (Central), `S` (Superior), `ST` (Superior Temporal)
  - `T` (Temporal), `IT` (Inferior Temporal), `I` (Inferior)
  - `IN` (Inferior Nasal), `N` (Nasal), `SN` (Superior Nasal)
- VariÃ¡veis descartadas: Index, pID, Age, Gender, Eye (nÃ£o relevantes para descobrir perfis)

**ImplementaÃ§Ã£o**:
```python
features = ['C', 'S', 'ST', 'T', 'IT', 'I', 'IN', 'N', 'SN']
df_selected = df[features]
```

---

### 2ï¸âƒ£ PrÃ©-processamento (Data Preprocessing)

**Objetivo**: Limpar e preparar os dados para anÃ¡lise.

**Neste projeto**:
- **Tratamento de valores ausentes**: RemoÃ§Ã£o de registros incompletos
- **VerificaÃ§Ã£o de outliers**: AnÃ¡lise de distribuiÃ§Ãµes
- **ValidaÃ§Ã£o de dados**: Garantir valores positivos e razoÃ¡veis

**Problemas encontrados e soluÃ§Ãµes**:
- Valores ausentes (NaN) â†’ Removidos
- ~0.5% dos dados removidos (mantendo 99.5% do dataset)

**ImplementaÃ§Ã£o**:
```python
df_clean = df_selected.dropna()  # Remove valores ausentes
```

---

### 3ï¸âƒ£ TransformaÃ§Ã£o (Data Transformation)

**Objetivo**: Transformar os dados em formato adequado para o algoritmo de mineraÃ§Ã£o.

**Neste projeto**:
- **NormalizaÃ§Ã£o**: StandardScaler (z-score normalization)
  - Transforma cada variÃ¡vel para mÃ©dia 0 e desvio padrÃ£o 1
  - Essencial para K-Means (algoritmo baseado em distÃ¢ncia)
  
**Por que normalizar?**
- K-Means usa distÃ¢ncia euclidiana
- VariÃ¡veis com diferentes escalas podem dominar o cÃ¡lculo
- Todas as regiÃµes tÃªm importÃ¢ncia similar (40-70 Î¼m)

**ImplementaÃ§Ã£o**:
```python
scaler = StandardScaler()
data_normalized = scaler.fit_transform(df_clean)
```

**Resultado**:
- MÃ©dia: 0.000000
- Desvio padrÃ£o: 1.000000
- Todas as variÃ¡veis na mesma escala

---

### 4ï¸âƒ£ MineraÃ§Ã£o de Dados (Data Mining)

**Objetivo**: Aplicar algoritmos de aprendizado de mÃ¡quina para descobrir padrÃµes.

**Neste projeto**:
- **Algoritmo**: K-Means Clustering
- **Por que K-Means?**
  - Adequado para dados numÃ©ricos
  - Eficiente para grandes volumes
  - Resultados interpretÃ¡veis (centrÃ³ides = perfis mÃ©dios)
  - NÃ£o-supervisionado (descoberta de padrÃµes)

**OtimizaÃ§Ã£o do K**:
Antes de aplicar K-Means, otimizamos o nÃºmero de clusters usando 4 mÃ©tricas:

1. **MÃ©todo do Cotovelo (Elbow Method)**
   - Analisa a inÃ©rcia vs nÃºmero de clusters
   - Procura o "cotovelo" na curva

2. **Silhouette Score** (0.45 - 0.75 = bom)
   - Mede quÃ£o similar um ponto Ã© ao seu cluster vs outros
   - Varia de -1 a 1 (quanto maior, melhor)

3. **Calinski-Harabasz Score**
   - RazÃ£o da dispersÃ£o entre clusters / dentro dos clusters
   - Quanto maior, melhor

4. **Davies-Bouldin Score**
   - MÃ©dia da similaridade entre cada cluster e seu mais similar
   - Quanto menor, melhor

**ImplementaÃ§Ã£o**:
```python
# OtimizaÃ§Ã£o
optimizer = KOptimizer(k_range=(2, 11))
df_metrics, k_optimal = optimizer.optimize(df_clean)

# ClusterizaÃ§Ã£o
kmeans = KMeans(n_clusters=k_optimal, n_init=10, random_state=42)
labels = kmeans.fit_predict(data_normalized)
```

**ParÃ¢metros do K-Means**:
- `n_clusters`: nÃºmero de clusters (otimizado)
- `n_init=10`: 10 execuÃ§Ãµes com diferentes inicializaÃ§Ãµes
- `random_state=42`: reprodutibilidade

---

### 5ï¸âƒ£ InterpretaÃ§Ã£o/AvaliaÃ§Ã£o (Interpretation/Evaluation)

**Objetivo**: Interpretar os padrÃµes descobertos e avaliar sua qualidade.

**Neste projeto**:

#### AnÃ¡lise Quantitativa:
- Tamanho de cada cluster (nÃºmero de olhos)
- Perfis mÃ©dios de espessura (centrÃ³ides)
- Desvios padrÃ£o (variabilidade)
- Assimetrias (Superior-Inferior, Temporal-Nasal)

#### AnÃ¡lise Qualitativa:
- InterpretaÃ§Ã£o clÃ­nica dos perfis
- IdentificaÃ§Ã£o de caracterÃ­sticas distintivas
- NomeaÃ§Ã£o dos clusters (ex: "fino", "normal", "espesso")

#### VisualizaÃ§Ãµes Geradas:
1. **Radar Charts**: Perfis espaciais dos clusters
2. **Boxplots**: DistribuiÃ§Ãµes por regiÃ£o
3. **Heatmaps**: CorrelaÃ§Ãµes e mapas espaciais
4. **GrÃ¡ficos de assimetria**: Superior-Inferior, Temporal-Nasal
5. **InterpretaÃ§Ã£o clÃ­nica**: Texto explicativo

**Exemplo de interpretaÃ§Ã£o**:
```
Cluster 0 (45% dos olhos):
- Espessura NORMAL (mÃ©dia: 54.2 Î¼m)
- PadrÃ£o UNIFORME (C vs P: 0.8 Î¼m)
- Simetria S-I PRESERVADA (+1.2 Î¼m)
- HOMOGÃŠNEO (DP: 3.5 Î¼m)
```

---

## ğŸ”„ Iteratividade do KDD

O KDD nÃ£o Ã© linear! Podemos retornar a etapas anteriores:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SeleÃ§Ã£o    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PrÃ©-proc.    â”‚ â† Voltar se houver problemas
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TransformaÃ§Ã£oâ”‚ â† Testar outras normalizaÃ§Ãµes
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MineraÃ§Ã£o    â”‚ â† Testar outros K ou algoritmos
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ InterpretaÃ§Ã£oâ”‚ â† Avaliar e refinar
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Exemplos de iteraÃ§Ã£o neste projeto**:
- Se K=3 nÃ£o for satisfatÃ³rio â†’ Testar K=4 ou K=5
- Se normalizaÃ§Ã£o nÃ£o funcionar â†’ Testar Min-Max ou Robust Scaler
- Se clusters nÃ£o fizerem sentido â†’ Revisar seleÃ§Ã£o de variÃ¡veis

---

## ğŸ“Š MÃ©tricas de AvaliaÃ§Ã£o Usadas

### 1. Silhouette Score
$$s(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}}$$

- `a(i)`: distÃ¢ncia mÃ©dia intra-cluster
- `b(i)`: distÃ¢ncia mÃ©dia ao cluster mais prÃ³ximo
- InterpretaÃ§Ã£o:
  - `s > 0.7`: Estrutura forte
  - `0.5 < s < 0.7`: Estrutura razoÃ¡vel
  - `0.25 < s < 0.5`: Estrutura fraca
  - `s < 0.25`: Sem estrutura

### 2. Calinski-Harabasz Score
$$CH = \frac{SS_B / (k-1)}{SS_W / (n-k)}$$

- `SS_B`: soma dos quadrados entre clusters
- `SS_W`: soma dos quadrados dentro dos clusters
- Maior valor = melhor separaÃ§Ã£o

### 3. Davies-Bouldin Score
$$DB = \frac{1}{k} \sum_{i=1}^{k} \max_{i \neq j} \left( \frac{\sigma_i + \sigma_j}{d(c_i, c_j)} \right)$$

- `Ïƒ_i`: dispersÃ£o mÃ©dia do cluster i
- `d(c_i, c_j)`: distÃ¢ncia entre centrÃ³ides
- Menor valor = melhor separaÃ§Ã£o

### 4. InÃ©rcia (Within-Cluster Sum of Squares)
$$I = \sum_{i=1}^{k} \sum_{x \in C_i} ||x - \mu_i||^2$$

- Soma das distÃ¢ncias quadradas aos centrÃ³ides
- Menor valor = clusters mais compactos
- Usado no mÃ©todo do cotovelo

---

## ğŸ¯ Diferenciais da ImplementaÃ§Ã£o

1. **Modularidade**: CÃ³digo organizado em classes reutilizÃ¡veis
2. **Seguimento rigoroso do KDD**: Cada etapa claramente separada
3. **MÃºltiplas mÃ©tricas**: DecisÃ£o baseada em consenso
4. **VisualizaÃ§Ãµes profissionais**: Adequadas para apresentaÃ§Ã£o clÃ­nica
5. **InterpretaÃ§Ã£o clÃ­nica**: TraduÃ§Ã£o dos resultados tÃ©cnicos para linguagem mÃ©dica
6. **Reprodutibilidade**: `random_state=42` em todos os lugares
7. **DocumentaÃ§Ã£o**: CÃ³digo comentado e mensagens informativas

---

## ğŸ“š ReferÃªncias

- Fayyad, U., Piatetsky-Shapiro, G., & Smyth, P. (1996). *From data mining to knowledge discovery in databases*. AI magazine, 17(3), 37-37.

- Han, J., Kamber, M., & Pei, J. (2011). *Data mining: concepts and techniques*. Elsevier.

- Kaufman, L., & Rousseeuw, P. J. (2009). *Finding groups in data: an introduction to cluster analysis*. John Wiley & Sons.

- Rousseeuw, P. J. (1987). *Silhouettes: a graphical aid to the interpretation and validation of cluster analysis*. Journal of computational and applied mathematics, 20, 53-65.

---

**Elaborado para a disciplina de Machine Learning - UFAL**
